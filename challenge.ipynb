{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Wikipedia...\n",
      "Searching DuckDuckGo...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# LLM 설정 (OpenAI API)\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 파일 저장 함수\n",
    "def save_the_file(docs):\n",
    "    with open(\"research_result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(docs)\n",
    "    return \"Research saved to research_result.txt\"\n",
    "\n",
    "# Wikipedia 검색 기능\n",
    "def search_wikipedia(keyword):\n",
    "    result = \"Wikipedia Results\\n\\n\"\n",
    "    retriever = WikipediaRetriever(top_k_results=3, lang=\"ko\")\n",
    "    data_list = retriever.get_relevant_documents(keyword)\n",
    "    \n",
    "    for page_content in data_list:\n",
    "        result += f\"{page_content.page_content}\\n\\n\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Wikipedia 도구 정의\n",
    "class WikipediaToolArgsSchema(BaseModel):\n",
    "    keyword: str = Field(description=\"Search keyword for Wikipedia\")\n",
    "\n",
    "class WikipediaTool(BaseTool):\n",
    "    name = \"Wikipedia\"\n",
    "    description = \"\"\"\n",
    "    A tool to search Wikipedia for a specific keyword. The 'keyword' argument is required.\n",
    "    \"\"\"\n",
    "    args_schema: Type[WikipediaToolArgsSchema] = WikipediaToolArgsSchema\n",
    "\n",
    "    def _run(self, keyword):\n",
    "        result = search_wikipedia(keyword)\n",
    "        return result\n",
    "\n",
    "# DuckDuckGo 검색 기능 (requests 사용) 및 웹사이트 텍스트 추출\n",
    "def search_duckduckgo(query):\n",
    "    url = \"https://html.duckduckgo.com/html/\"\n",
    "    params = {'q': query}\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    response = requests.post(url, data=params, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    results = \"\"\n",
    "    urls = []\n",
    "    for a in soup.find_all('a', {'class': 'result__a'}, href=True):\n",
    "        results += f\"{a['href']}\\n\"\n",
    "        urls.append(a['href'])  # DuckDuckGo에서 URL을 수집\n",
    "    \n",
    "    return results, urls\n",
    "\n",
    "# 웹사이트에서 텍스트 추출\n",
    "def extract_website_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # 사이트에서 텍스트만 추출\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text_content = \"\\n\".join([para.get_text() for para in paragraphs])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        return f\"Failed to extract content from {url}: {e}\"\n",
    "\n",
    "# DuckDuckGo 도구 정의\n",
    "class DuckDuckGoSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(description=\"Query for DuckDuckGo search\")\n",
    "\n",
    "class DuckDuckGoSearchTool(BaseTool):\n",
    "    name = \"DuckDuckGoSearchTool\"\n",
    "    description = \"\"\"\n",
    "    A tool to search DuckDuckGo for a specific query and return the results.\n",
    "    \"\"\"\n",
    "    args_schema: Type[DuckDuckGoSearchToolArgsSchema] = DuckDuckGoSearchToolArgsSchema\n",
    "\n",
    "    def _run(self, query):\n",
    "        results, urls = search_duckduckgo(query)\n",
    "        if urls:\n",
    "            # 첫 번째 URL에서 콘텐츠 추출\n",
    "            content = extract_website_content(urls[0])\n",
    "            return f\"DuckDuckGo Search Results:\\n{results}\\n\\nExtracted Content from {urls[0]}:\\n\\n{content}\"\n",
    "        else:\n",
    "            return \"No results found in DuckDuckGo\"\n",
    "\n",
    "# 에이전트 초기화\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        WikipediaTool(),\n",
    "        DuckDuckGoSearchTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 에이전트 실행 함수\n",
    "def run_research(query):\n",
    "    # Wikipedia에서 검색\n",
    "    print(\"Searching Wikipedia...\")\n",
    "    wikipedia_result = agent.tools[0]._run(query)\n",
    "    \n",
    "    # DuckDuckGo에서 검색 및 웹사이트 콘텐츠 추출\n",
    "    print(\"Searching DuckDuckGo...\")\n",
    "    duckduckgo_result = agent.tools[1]._run(query)\n",
    "    \n",
    "    # 결과를 파일에 저장\n",
    "    research_content = f\"{wikipedia_result}\\n\\n{duckduckgo_result}\"\n",
    "    save_the_file(research_content)\n",
    "\n",
    "# 에이전트 실행: \"Research about the XZ backdoor\"\n",
    "run_research(\"XZ backdoor\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
