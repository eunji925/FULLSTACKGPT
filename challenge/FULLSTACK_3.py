# %%
# 1. Implement an LCEL chain with a memory that uses one of the memory classes we learned about.
# 2. The chain should take the title of a movie and reply with three emojis that represent the movie. (i.e "Top Gun" -> "ğŸ›©ï¸ğŸ‘¨â€âœˆï¸ğŸ”¥". "The Godfather" -> "ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ğŸ”«ğŸ ").
# 3. Provide examples to the chain using FewShotPromptTemplate or FewShotChatMessagePromptTemplate to make sure it always replies with three emojis.
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate

#llmê³¼ memory ì„¤ì •í•˜ê¸°
llm = ChatOpenAI(temperature=0)

memory = ConversationBufferMemory(
    llm=llm,
    return_messages=True,
)

# %%
#example ë§Œë“¤ê¸°
examples = [
    {
        "question": "Spider Man",
        "answer": "ğŸ•·ï¸ğŸ•¸ï¸ğŸ—½",
    },
    {
        "question": "Iron Man",
        "answer": "ğŸ¦¾ğŸ•¶ï¸ğŸ”¥",
    },
    {
        "question": "Thor",
        "answer": "âš¡ï¸ğŸ”¨ğŸŒ©ï¸",
    },
]

# %%
#exampleë¥¼ ì œê³µí•˜ê³  memory ê¸°ë¡ì„ ì´ìš©í•œ í”„ë¡¬í”„íŠ¸ ë§Œë“¤ê¸°
example_prompt = ChatPromptTemplate.from_messages(
    [("human", "{question}"), ("ai", "{answer}")]
)

fewshot_chat_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

final_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a movie expert. You know every movie. If a human tells you the title of the movie, you have to respond with 3 emoticons.",
        ),
        fewshot_chat_prompt,
        (
            "system",
            "The above examples should not be provided to the user. The user can only be provided with the conversation record below. Please provide the information to the user using the record below.",
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)

# %%
#chain ë§Œë“¤ê¸°
def load_memory(_):
    return memory.load_memory_variables({})["history"]


chain = RunnablePassthrough.assign(history=load_memory) | final_prompt | llm


def invoke_chain(question):
    result = chain.invoke({"question": question})
    memory.save_context({"input": question}, {"output": result.content})
    print(result)

# %%
#ì˜í™” ì œëª© ì…ë ¥ í…ŒìŠ¤íŠ¸
invoke_chain("Captain America")
invoke_chain("Mission Impossible")

# %%
#ì²˜ìŒ ì§ˆë¬¸í•œ ì˜í™” ì…ë ¥ í…ŒìŠ¤íŠ¸
invoke_chain("What was the first movie I asked?")


