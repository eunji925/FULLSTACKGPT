{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"According to the context provided, Winston accepts that Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with. However, it is also mentioned that he had seen a photograph that disproved their guilt, but he convinces himself that the photograph never existed and that he had invented it. This suggests that Aaronson's guilt is questionable and possibly fabricated by the Party.\"\n"
     ]
    }
   ],
   "source": [
    "# 1. Implement a complete RAG pipeline with a Stuff Documents chain.\n",
    "# 2. You must implement the chain manually.\n",
    "# 3. Give a ConversationBufferMemory to the chain.\n",
    "# 4. Use this document to perform RAG: \"./rag_files/document.txt\"\n",
    "# 5. Ask the following questions to the chain: Is Aaronson guilty? / What message did he write in the table? / Who is Julia?\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 문서 로드와 쪼개기 / 임베딩 생성 및 캐시\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = TextLoader(\"./rag_files/document.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# 메모리와 문서를 이용한 프롬프트\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=20,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 체인 연결 / 체인 호출 함수 정의\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": load_memory,\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question)\n",
    "    print(result)\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "\n",
    "# 체인에 질문하여 테스트\n",
    "invoke_chain(\"Is Aaronson guilty?\")\n",
    "# invoke_chain(\"What message did he write in the table?\")\n",
    "# invoke_chain(\"Who is Julia?\")\n",
    "\n",
    "# # 메모리를 출력하여 메모리가 체인에 적용되었는지 확인\n",
    "# load_memory({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
